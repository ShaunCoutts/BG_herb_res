\documentclass[12pt, a4paper]{article}

\usepackage{setspace, graphicx, lineno, caption, color, float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amstext} %to enter text in mathematical formulae
\usepackage{algpseudocode}%psuedo code and 
\usepackage{algorithm}%puts the psuedo code in a float box
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage{natbib}
%\usepackage{url, hyperref, makeidx, fancyhdr, booktabs, palatino}
%\usepackage{euscript} %EuScript, command: \EuScript, for letters in Euler script
%\usepackage{paralist} %listing (i), (ii), etc.
\usepackage{rotating} %rotating text
\usepackage{multirow} %connecting columns in tables
\usepackage{multicol}
\usepackage{longtable}
%image stuff
%\usepackage{epstopdf}
%\usepackage{cancel}
%\usepackage[ngerman, english]{babel} %for different languages in one document
%\usepackage[utf8]{inputenc}
%\hypersetup{colorlinks=true, linkcolor=blue}
%page set up

\usepackage[left=2.5cm,top=2.5cm,right=2.5cm, bottom=2.5cm,nohead]{geometry}
\doublespacing
%paragraph formatting
\setlength{\parskip}{12pt}
\setlength{\parindent}{0cm}

\makeatletter
\let\OldStatex\Statex
\renewcommand{\Statex}[1][3]{%
  \setlength\@tempdima{\algorithmicindent}%
  \OldStatex\hskip\dimexpr#1\@tempdima\relax}
\makeatother

\begin{document}
%Make a title page
\title{Managing the inevitable rise of herbicide resistant}
\author{Shaun R. Coutts$^\ddag$, Rob Freckleton$^\dag$, Dylan Childs$^\dag$, }
\maketitle
\section{non-spatial IPM}
We develop a simple model of herbicide resistance in black grass, where resistance is assumed to be a quantitative trait. We model the management of black grass under the evolution of herbicide resistance, where chemical control leads to herbicide resistance with prolonged use, and cultural control which is more costly but does not lead to evolved resistance. We find the optimal combination of control options over a finite time horizon, aiming either to maximize net present economic value or minimize resistance with a fixed budget. We test under what conditions would a profit maximizing manager act in a way that avoided selecting for herbicide resistance. We also search for a set of actions that can meaningfully reduce herbicide resistance for a reasonable cost. This might apply in cases where profit is not the main motivator, such as control of invasive species on public lands.                  

We will start by describing the system model and then the optimization process.

\subsection{System model}
We model the black grass population using a yearly time step, starting at the beginning of the growing season before any seeds have emerged. We track the number of seeds in the seed bank with resistance $g$ at depth $d$ given action $a_k$ is taken. The action $a_k$ is how the manager can affect the population model, and thus the reward they get from taking a given set of actions. Each action is a tuple of five sub-actions $a_k = \langle a_h, a_b, a_m, a_c, a_d \rangle$, see Table \ref{table:actions} for a description of the sub-actions and Appendix 1 for all the eligible combinations of these sub-actions (i.e. the full actions space).      

The seed bank at time $t$ is the seeds already in the seed bank at depth $d$ that don't germinate and survive one time step (first term in Eq. \ref{eq:seedbank_top}), plus the number of seeds added to the top level seed bank given action $a_k$ was taken, $f(g, t, a_k)$. We created two versions of the model, one with a simple one level seed bank (Eq. \ref{eq:seedbank_simp}) and a structured seed bank with two levels (Eq. \ref{eq:seedbank_struc}). The distribution of seeds in the single layer seed bank across resistance score $g$ was was always approximately normal. This predictable, simple distribution allowed us to use approximate dynamic programming to find optimal management strategies. When the seed bank had two layers many more distributions across $g$ became possible, some being highly skewed and even multi-modal. In these cases we could could only find sub-optimal (but still good) management strategies via genetic algorithm. In the simple single layer seed bank the distribution of seeds in the seed bank across $g$ at time $t$ is given by   
\begin{equation}\label{eq:seedbank_simp}
	b(g, t + 1) = [b(g, t) - b(g, t)p(a_b)](1 - \phi_e)\phi_b + f(g, t, a_k) 
\end{equation}
In this simple model we assume that once seeds leave the top level of the seed bank they are destroyed. The proportion of seeds moved out of the top layer of the seed bank in response to plowing is 
\begin{equation}\label{plowing_simple}
	p(a_b) = \begin{cases}
		\vartheta~\text{if: }a_b = \text{plow}\\
		0~\text{if: }a_b = \text{no\_plow}
	\end{cases} 
\end{equation}
Where $\vartheta \in [0, 1]$ is the reduction in viable seeds caused by plowing. Eq. \ref{plowing_simple} ties actions that manage the seed bank $a_b \in \{\text{plow}, \text{no\_plow}\}$, to the population model. In the more realistic multi-level seed bank
\begin{subequations}\label{eq:seedbank_struc}
	\begin{equation}\label{eq:seedbank_top}
	\begin{split}
		b(g, d = 1, t + 1) = [b&(g, d = 1, t) - b(g, d = 1, t)p(1 \rightarrow 2|a_b) + \\
		&b(g, d = 2, t)p(2 \rightarrow 1|a_b)](1 - \phi_e)\phi_b + f(g, t, a_k) 
	\end{split}
	\end{equation}
	\begin{equation}\label{eq:seedbank_bottom}
	\begin{split}	
		b(g, d = 2, t + 1) = [b(g, d = 2, t) - b(g, d& = 2, t)p(2 \rightarrow 1|a_b) + \\
		 &b(g, d = 1, t)p(1 \rightarrow 2|a_b)]\phi_b 
	\end{split}
	\end{equation}
\end{subequations}
where $b(g, d, t)$ is the distribution of seeds over $g$ at depth class $d \in \{1, 2\}$ at time $t$. $\phi_b$ is the probability that a seed survives and $p(d' \rightarrow d|a_b)$ is the proportion of seeds that move from depth class $d'$ to depth class $d$ given action to manage the seed bank $a_b$.    
\begin{equation}
	p(d' \rightarrow d|a_b) = \begin{cases}
		\varrho &\text{if}~a_b = \text{plow} \\
		0 &\text{if}~a_b = \text{no\_plow}
	\end{cases}
\end{equation}  
where $\varrho$ is the proportion of seeds moved between depth levels by plowing. 
   
The distribution of new seeds over resistance $g$, $f(g, t, a_k)$, is modelled as the offspring produced by the joint distribution of maternal and paternal survivors given the sub-actions in tuple $a_k$ (see Table \ref{table:actions}). In the following we use the two level structured seed bank, but it is equivalent to the single level seed bank model when we replace $[b(g, d = 1, t) - b(g, d = 1, t)p(1 \rightarrow 2|a_b) + b(g, d = 2, t)p(2 \rightarrow 1|a_b)]$ with $[b(g, t) - b(g, t)p(a_b)]$ in equation \ref{eq:above_ground}.        
\begin{equation}
\label{eq:fecund}
\begin{split}
	f(g, t, a_k) = \displaystyle&\int_{g_m}\int_{g_p} \text{N}(0.5 g_m + 0.5 g_p, \sigma_f)s(g_m, t, a_h, a_c)\psi(g_m,a_d)s(g_f, t, a_h, a_c)\times \\ 
		&~~~~~~m(a_m)^2 n(g_m, t, a_b)n(g_p, t, a_b)\varsigma^2\text{d}g_m\text{d}g_p + \\
		&\int_{g_m}\int_{g_p} \text{N}(0.5 g_m + 0.5 g_p, \sigma_f)s^*(g_m, t, a_c)\psi(g_m,a_d)s^*(g_f, t, a_c)\times \\ 
		&~~~~~~m(a_m)^2 n(g_m, t, a_b)n(g_p, t, a_b)(1 - \varsigma)^2 \text{d}g_m \text{d}g_p 
\end{split}
\end{equation}

We assume that the maternal and paternal distributions are the same (i.e. $n(g_m, t, a_b) = n(g_f, t, a_b) = n(g, t, a_b)$), since individuals produce both seeds and pollen. The offspring produced by every pair of $g_m$:$g_f$ values are assumed to be normally distributed over $g$ with a mean of $0.5g_m + 0.5g_f$ and a standard deviation of $\sigma_f$ (first term in Eq. \ref{eq:fecund}). We assume that only seeds in the top layer of the seed bank germinate, thus the number of individuals that emerge from the seed bank is
\begin{equation}\label{eq:above_ground}
	n(g, t, a_b) = [b(g, d = 1, t) - b(g, d = 1, t)p(1 \rightarrow 2|a_b) + b(g, d = 2, t)p(2 \rightarrow 1|a_b)]\phi_e\phi_b
\end{equation}
where $\phi_e$ is the germination rate from the top of the soil profile. We assume that not all seeds emerge at the same time so that only portion $\varsigma$ of the individuals are exposed to the herbicide if it is applied. The probability that those germinated individuals exposed to herbicide survive long enough to contribute seeds and pollen to the next generation given sub-actions $a_h$ and $a_c$ are taken is 
\begin{align}\label{eq:survival_herb}
	s(g, t, a_h, a_c) =& \frac{s_\text{max}}{1 + e^{-\Phi_s}}\\
	\Phi_s =& s_0 - s_rg - h(a_h)\left(\xi_h - \textbf{min}(\xi_h, \rho g) \right) - c(a_c)\\
	h(a_h) =& \begin{cases}
		1 &\text{if}~a_h = \text{herb}\\
		0 &\text{if}~a_h = \text{no\_herb}
	\end{cases}\\
	c(a_c) =& \begin{cases}
		0 &\text{if}~a_c = \text{wheat}\\
		\alpha_\text{alt} &\text{if}~a_c = \text{alt}\\
		\alpha_\text{fallow} &\text{if}~a_c = \text{fallow}
	\end{cases} 
\end{align}
where $s_\text{max}$ is the maximum survival rate for black grass. $s_0$ is the survival probability (in logits) when there is no herbicide for individuals with resistance score $g = 0$ and $\xi_h$ is the reduction in survival (in logits) caused by herbicide for individuals with a resistance score $g = 0$. $s_r$ is the survival cost of herbicide resistance (in logits) and $\rho$ is the protective effect of a one unit increase in resistance score $g$. $h(a_h)$ is a function that returns 0 or 1 depending on the action $a_h$. Survival under different crops is controlled by the function $c(a_c)$ which returns the reduction in survival (in logits), $\alpha_{a_c}$, given crop choice $a_c$. Survival for individuals that are not exposed to herbicide is   
\begin{align}\label{eq:survival_noherb}
	s^*(g, t, a_c) =& \frac{s_\text{max}}{1 + e^{-s_0 - s_rg - c(a_c)}}\\
\end{align}
$m(a_m)$ is the proportion of black grass individuals that survive mechanical control given sub-action $a_m$.
\begin{equation}
	m(a_m) = \begin{cases}
		\eta &\text{if}~a_m = \text{mech}\\
		1 &\text{if}~a_m = \text{no\_mech}
	\end{cases}\\
\end{equation}
Mechanical control is assumed to have a fixed efficacy $\eta \in [0, 1]$, but the cost of obtaining that efficacy increases with increasing above ground black grass populations (Eq. \ref{eq:cost_mech}).

The number of seeds produced per individual, $\psi(g, a_k)$, is a function of resistance, with greater resistance reducing the number of seeds produced, the density of surviving plants and the density of wheat planting, both influenced by multiple sub-actions in $a_k$. 
\begin{equation}\label{eq:seed_production}
	\psi(g, a_k) = \frac{z(a_d)z(a_c)\psi_\text{max}}{1 + e^{-(f_0 - f_rg)} + f_d M(a_k, t) + f_dM(a_k, t) e^{-(f_0 - f_rg)}}
\end{equation}  
where $\psi_\text{max}$ is the maximum possible number of seeds per individual, $f_0$ controls the number of seeds produced when $g = 0$, $z(a_u) \in [0, 1]$ is the seed mortality when sub-action $a_u$ is taken, $f_r$ is the cost of resistance in terms of reduction in seed production, $1/f_d$ is the population level where individuals start to interfere with each other and 
\begin{equation}\label{eq:num_ind}
	M(a_k, t) = \int \text{d}g~n(g, t, a_b)s(g, t, a_h, a_m, a_c)  
\end{equation}
is the number of above ground individuals that survive until seed set.

\subsection{Finding the optimal policy}
The goal is to find the set of actions for a given set of states (in optimization research called a policy and denoted with $\pi$) that maximize the value of the discounted cumulative reward function. The immediate reward function $R(x, a)$ gives the immediate reward of being in state $x$ and taking action $a$. State $x$ is a point in the continuous state domain $\mathbf{x}$, $x$ is based on the number of seeds in the seed bank and the resistance of those seeds $b(g, d)$. We run two separate optimization procedures approximate dynamic programming and on a simpler model. When we include a two level seed bank we search for the best sequence of actions from a given starting state using a genetic algorithm.       

In the single level seed bank we assume that $b(g, d)$ is normally distributed with a constant standard deviation $\sigma_b$. We estimate $\sigma_b$ before the optimization process starts by running the system model and observing the standard deviation over $g$ in $b(g, t)$. Assuming constant standard deviation means the whole state space, $\mathbf{x}$, is defined over two dimensions, the total number of seeds in the seed bank $B_d = \int b(g)\text{d}g$, and mean resistance score of those seeds $\overline{g}_d$. In the multi-level seed bank we make no assumptions about the distribution of the seed bank over $g$.   

A crucial part of any optimization process is defining the action space, that is the full set of actions of that can be taken in any time step. The action space is discrete and there are five types of sub-actions that can be combined at each time step, shown in Table \ref{table:actions}. 
\begin{longtable}[h]{c p{4.1cm} p{4.1cm} p{4.1cm}}
\caption{Management sub-actions available and their effects on the population model\label{table:actions}}\\
	\hline
	\textbf{Sub-action} & \textbf{Description} & \textbf{Effect on system model} & \textbf{Management parameters}\\
	\hline	
	\multicolumn{4}{l}{\textit{Crop choice}: $a_c$}\\
	wheat & plant wheat crop at optimal time and density (given there is no black grass) & higher survival rate for black grass and herbicide less effective due to spraying occurring earlier & Highest profit crop in the absence of black grass \\
	alt & alternative crop to wheat, could be a more competitive crop such as barley or wheat planted at a later time & black grass survival reduced under alternative crop during to competition or broad spectrum herbicide use before sowing & reduction of income compared to wheat\\
	fallow & no crop planted, left to grass & All above ground plants killed as we assume this crop choice is always used alongside plowing or broad spectrum herbicide & small negative income as their is no production for that year and there is a cost to killing all above ground plants\\
	\multicolumn{4}{l}{\textit{Herbicide}: $a_h$}\\
	herb & herbicide applied & herbicide reduces above ground survival depending on the resistance of the individuals & there is a fixed application cost\\ 
	no\_herb & herbicide & no effect & no cost\\
	\multicolumn{4}{l}{\textit{Seed bank management}: $a_b$}\\
	plow & plow field seedling pre-emergence & In the simple model plowing simply destroys some of the seeds, while in the multi-level seed bank model plowing moves seeds from one level of the seed bank to the other & there is cost to plowing\\
	no\_plow & no effect & no effect & no cost\\
	\multicolumn{4}{l}{\textit{Mechanical control}: $a_m$}\\ 
	mech & Mechanical control such as hand pulling, or spot hoing, it could also include tools such as spot spraying with a knock down herbicide & reduces survival but does not cause herbicide resistance & cost of mechanical control depends on the population size with a small fixed cost, will be expensive for large black grass populations.\\
	no\_mech & no effect & no effect & no effect\\
	\multicolumn{4}{l}{\textit{Planting density}: $a_d$}\\ 
	optim\_den & optimal planting density of wheat in the absence of wheat & no effect & no cost\\
	high\_den & high density wheat planting & reduced black grass fecundity & reduction in yield from optimal planting density\\ 
	\hline
\end{longtable}
A single action is a tuple of these five sub-action, $a_k = \langle a_b, a_h, a_m, a_c, a_d \rangle$, and the action space are the possible combinations of these sub-actions. Not every combination of sub actions makes sense, for example some actions may only make sense under the crop choice wheat. The action space we use is defined in Appendix 1.

For direct search we try and maximize value function, $V(x_0|\mathbf{a})$, that gives the accumulated rewards of starting in state $x_0$ and following the action sequence $\mathbf{a} = \{a_k^1, a_k^2, ... a_k^T\}$, where $a_k^t$ is the action taken at time $t$. We use two different types of value function that represent different motivations. The first value function (Eq. \ref{rewards:ds_NPV}) focuses on a purely profit driven decision maker and is the discounted income. 
\begin{equation}\label{rewards:ds_NPV}
	V(x_0|\mathbf{a}) = \sum_{t = 0}^{T} \gamma^t R(x_t, a_k^t) 
\end{equation}
Where $\gamma$ is the discount rate and 
\begin{equation}\label{rewards:immediate}
	R(x_t, a_k^t) = \text{max}\left[0, y_{a_c} - \varphi_{a_c} M(a_k^t, x_t)\right] - \sum_{\forall a_u \in a_k^t} \theta(a_u)  
\end{equation}
where $y_{a_c}$ is the expected income from crop choice $a_c$ (taken from tuple $a_k^t$). $\varphi_{a_c}$ is the loss in yield of crop type $a_c$ from each black grass individual. Recall that $M(a_k^t, x_t)$ is the total number of above ground individuals after management (Eq. \ref{eq:num_ind}). We replace the time variable $t$ with the state variable $x_t$ since $M(a_k^t, x_t)$ can calculated from the information contained in the state variable $x_t$. We assume there is no income from leaving the field fallow, and also that there is no yield loss, i.e. $y_\text{fallow} = \varphi_\text{fallow} = 0$. $\theta(a_u)$ is the cost of doing sub-action $a_u$, $a_u \in \{\text{plow}$, $\text{no\_plow}$, $\text{herb}$, $\text{no\_herb}$, $\text{mech}$, $\text{no\_mech}$, $\text{optim}$, $\text{high\_den}$, $\text{wheat}$, $\text{alt}$, $\text{fallow}\}$. In the case of plowing, planting density, crop choice and herbicide sub-actions $\theta(a_u)$ is constant. For mechanical control we assume that the cost of control increases with increasing weed numbers (which is in part controlled by the other sub-actions taken).
\begin{equation}\label{eq:cost_mech}
	\theta(a_m) = \begin{cases}
	\kappa_0 + \kappa \int \text{d}g~ n(g, t, a_b)\phi_s(a_h, a_c, g) &\text{if } a_m = \text{mech}\\
	0 &\text{if } a_m = \text{no\_mech}
	\end{cases} 
\end{equation}    
Where $\kappa_0$ is the cost of mechanical control when there are no black grass present (e.g. search costs, equipment costs) and $\kappa$ is the increase in cost for every black grass individual in the population. The integral sums the total number of above ground black grass individuals after plants establish out of the seed bank (Eq. \ref{eq:above_ground}) and after other above ground sub-actions have been taken (Eq. \ref{eq:sur_dens_ind}). This assumes that mechanical control happens after all other management sub-actions. 

The second reward function represents a manager tasked with minimizing the mean resistance score, $\overline{g}$, of the black grass population, with the constraint that only budget $\beta$ can be spent in any one time step. 
\begin{equation}\label{rewards:ds_g_min}
	R(x_t, a_k^t, t) = \begin{cases}
	-\overline{g}_T &\text{if } t = T \bigwedge \sum_{\forall a_u \in a_k^t} \theta(a_u) \leq \beta \\
	0 &\text{if } t < T \bigwedge \sum_{\forall a_u \in a_k^t} \theta(a_u) \leq \beta \\
	-\infty &\text{if } \sum_{\forall a_u \in a_k^t} \theta(a_u) > \beta     
	\end{cases}  
\end{equation}

The goal now is to find the sequence of actions $\mathbf{a}$ that maximizes $V(x_0|\mathbf{a})$. This is a combinatorial optimization problem, which are classically solved using a genetic algorithm. Starting with population, $P$ of random sequences we evaluate the value function (Eq. \ref{rewards:ds_NPV} or \ref{rewards:ds_g_min}) for each $\mathbf{a}_i \in P = \{\mathbf{a}_1, \mathbf{a}_2, ..., \mathbf{a}_{P_\text{size}}\}$. The $P^*$ action sequences with the best value are allowed to be parents to the next generation of action sequences. These sequences are recombined with each other using a cross over process to form new action sequences. These new action sequences also experience mutations that randomly switch one action in the sequence to another. These action sequences now make up the population $P$ and the process is repeated until a single action sequence comes to dominate the entire population. This action sequence will be near optimal, although there are no guarantees that this is a global optimum. We can restart the genetic algorithm with a new initial $P$ to see if that starting set of action sequences reaches the same optimal action sequence. 

In an alternative approach we can formulate this problem as a Markov Decision Process (MDP), where the state at each time step is determined only by the state and action take in the last time step. MDPs can be solved using a powerful set of tools called dynamic programming to find the optimal policy $\pi^*$. A policy is a mapping of states to actions, so that in any state the system can be in the policy suggests an action to take in that policy. $\pi^*$ then is the policy where the action suggested in each state is the one that maximizes the value of being in that state. To find the optimal policy we use a version of dynamic programming called heuristic dynamic programming \citep{Werb1992}, solved using backward iterations of the Bellman equation 
\begin{equation}\label{eq:bellman}
	V(x) = \max\limits_{a \in A}\lbrace R(x, a) + \gamma V(x'|a, x) \rbrace 
\end{equation}                  
Where $V(x)$ is the value of being in state $x$ and $V(x'|a, x)$ is the value of the state the system will be in at the next time step, $x'$, given the current state is $x$ and action $a$ is taken. $x'$ is calculated using the system model in Eq. \ref{eq:seedbank}. $R(x, a)$ and $\gamma$ are defined in Eq. \ref{rewards:ds_NPV}. The goal of dynamic programing is to find $V(x)$ for all $x \in \mathbf{x}$. We can do this efficiently by making three strong assumptions; The next state is completely determined by the current state and the action taken (system is memory-less), all future actions will be optimal, and we do not care about rewards obtained after time horizon $T$. These three assumptions mean we can use a simple version of backward iteration to find $\pi^*$. The only complication is the state space is continuous and multi-dimensional, thus we evaluate $V(x)$ at a set of points and use a spline to interpolate between those evaluation points, $V(\mathbf{x})$ is actually a spline that approximates the real value function. We do this using Algorithm \ref{alg:HDP}.

The only difference between Algorithm \ref{alg:HDP} and standard dynamic programming via backward iteration is the need to generate a set of points, $\hat{\mathbf{x}}$ and fit a spline ($Q$) to the values calculated for each evaluated state. This is required because our state space is a continuous 2D surface and we need a means of approximating that surface.

{\setstretch{1.0}
\begin{algorithm}[H]
\caption{Heuristic Dynamic Programming algorithm using backward iteration}
\label{alg:HDP}
\begin{algorithmic}[1]
	\State Use Latin hyper-cube sampling to generate a set of evaluation points $\hat{\mathbf{x}}$. 	
	\ForAll{$x \in \hat{\mathbf{x}}$} 
		\State $\hat{V}(x) = \max\limits_{a \in A} R(x, a)$ 
	\EndFor
	\State Fit spline $Q$ to the values of the evaluated states $\hat{V}(\hat{\mathbf{x}})$ to approximate the value 			
	\Statex[1] function, $V(\mathbf{x}) = Q$.
	\For{$t = T$ \textbf{to} $t = 1$}
		\State Use Latin hyper-cube sampling to generate a set of evaluation points $\hat{\mathbf{x}}$ 
		\ForAll{$x \in \hat{\mathbf{x}}$}
			\State Generate normal distributions $b(g, d = 1, t) = x\langle B_1 \rangle \text{N}(x\langle \overline{g}_1\rangle, \sigma_b)$ and 
			\Statex[3] $b(g, d = 2, t) = x\langle B_2 \rangle \text{N}(x\langle \overline{g}_2\rangle, \sigma_b)$
			\State $\hat{V}(x) \gets -\infty$
			\ForAll{$a \in A$}
				\ForAll{$d \in \{1,2\}$}
					\State Use $b(g, d, t)$ to calculate $b(g, d, t + 1)$ using Eq. \ref{eq:seedbank} (note action $a$ affects 
					\Statex[5]this calculation).
				\EndFor
				\State Encode state $x' = \langle B_1 = \int b(g, d = 1, t + 1)\text{d}g, \overline{g}_1 =\text{mean}(b(g, d = 1, t + 1)),$
				\Statex[4]$B_2 = \int b(g, d = 2, t + 1)\text{d}g, \overline{g}_2 = \text{mean}(b(g, d = 2, t + 1)) \rangle$
				\State Predict value of $x'$ using spline $Q$: $V(x'|a, x) = Q(x')$
				\State Calculate the tentative state value $\hat{V}(x)_\text{tet} =  R(x, a) + \gamma V(x'|a, x)$ 
				\If{$\hat{V}(x)_\text{tet} > \hat{V}(x)$} 
					\State$\hat{V}(x) \gets \hat{V}(x)_\text{tet}$
					\State $\pi_t^*(x) \gets a$
				\EndIf   
			\EndFor
		\EndFor
		\State Fit spline $Q$ to the values of the evaluated states $\hat{V}(\hat{\mathbf{x}})$ to approximate the value 
	\EndFor
\end{algorithmic}
\end{algorithm}  
}

The genetic algorithm and the dynamic programming achieve similar goals but offer different strengths. HDP is more efficient and finds the policy across all states, not just a few states, also HDP is likely to find a better policy (if $Q$ is a good representation of $V(\mathbf{x})$). However HDP will only show the single optimal policy, there may be a much simpler policy that is almost as good, the genetic algorithm will show a set of good (but not quiet optimal) action sequences. Also HGP will not do well if the value function is very bumpy, as in that case we will need lots of evaluation at each iteration to capture its shape. The Genetic algorithm will be slower but will work better if the solution space is very bumpy.       

\newpage
\section*{Appendix 1}
\begin{longtable}[h]{c c c c c c}
\caption{Action space with all eligable combinations of sub actions\label{table:action_space}}\\
	\hline
	$\mathbf{a_k}$ & $\mathbf{a_h}$ & $\mathbf{a_b}$ & $\mathbf{a_c}$ & $\mathbf{a_m}$ & $\mathbf{a_d}$\\
	\hline
	$a_1$ & wheat & herb & plow & mech & optim\\
	$a_2$ & wheat & herb & plow & mech & high\_den\\
	$a_3$ & wheat & herb & plow & no\_mech & optim\\
	$a_4$ & wheat & herb & plow & no\_mech & high\_den\\
	$a_5$ & wheat & herb & no\_plow & mech & optim\\
	$a_6$ & wheat & herb & no\_plow & mech & high\_den\\
	$a_7$ & wheat & herb & no\_plow & no\_mech & optim\\
	$a_8$ & wheat & herb & no\_plow & no\_mech & high\_den\\
	$a_9$ & wheat & no\_herb & plow & mech & optim\\
	$a_{10}$ & wheat & no\_herb & plow & mech & high\_den\\
	$a_{11}$ & wheat & no\_herb & plow & no\_mech & optim\\
	$a_{12}$ & wheat & no\_herb & plow & no\_mech & high\_den\\
	$a_{13}$ & wheat & no\_herb & no\_plow & mech & optim\\
	$a_{14}$ & wheat & no\_herb & no\_plow & mech & high\_den\\
	$a_{15}$ & wheat & no\_herb & no\_plow & no\_mech & optim\\
	$a_{16}$ & wheat & no\_herb & no\_plow & no\_mech & high\_den\\
	$a_{17}$ & alt & herb & plow & mech & optim\\
	$a_{18}$ & alt & herb & plow & no\_mech & optim\\
	$a_{19}$ & alt & herb & no\_plow & mech & optim\\
	$a_{20}$ & alt & herb & no\_plow & no\_mech & optim\\
	$a_{21}$ & alt & no\_herb & plow & mech & optim\\
	$a_{22}$ & alt & no\_herb & plow & no\_mech & optim\\
	$a_{23}$ & alt & no\_herb & no\_plow & mech & optim\\
	$a_{24}$ & alt & no\_herb & no\_plow & no\_mech & optim\\
	$a_{25}$ & fallow & no\_herb & no\_plow & no\_mech & optim\\
	\hline
\end{longtable}


\bibliographystyle{/home/shauncoutts/Dropbox/shauns_paper/referencing/bes} 
\bibliography{/home/shauncoutts/Dropbox/shauns_paper/referencing/refs}

\end{document}